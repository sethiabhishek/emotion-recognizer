{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "import os\n",
    "import csv\n",
    "import argparse\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'fer2013.csv'\n",
    "image_size = (48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fer2013():\n",
    "    data = pd.read_csv(dataset_path)\n",
    "    pixels = data['pixels'].tolist()\n",
    "    width, height = 48, 48\n",
    "    faces = []\n",
    "    for pixel_sequence in pixels:\n",
    "        face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
    "        face = np.asarray(face).reshape(width, height)\n",
    "        face = cv2.resize(face.astype('uint8'),image_size)\n",
    "        faces.append(face.astype('float32'))\n",
    "    faces = np.asarray(faces)\n",
    "    faces = np.expand_dims(faces, -1)\n",
    "    emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
    "    return faces, emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x\n",
    " \n",
    "faces, emotions = load_fer2013()\n",
    "faces = preprocess_input(faces)\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printimages():\n",
    "    \n",
    "    w, h = 48, 48\n",
    "    image = np.zeros((h, w), dtype=np.uint8)\n",
    "    id = 1\n",
    "    output_folder = 'original_images'\n",
    "    with open(dataset_path) as csvfile:\n",
    "        datareader = csv.reader(csvfile, delimiter =',')\n",
    "        next(datareader,None)\n",
    "\t\n",
    "        for row in datareader:\n",
    "        \n",
    "            emotion = row[0]\n",
    "            pixels = row[1].split()\n",
    "            usage = row[2]\n",
    "            pixels_array = np.asarray(pixels, dtype=np.int)\n",
    "\n",
    "            image = pixels_array.reshape(w, h)\n",
    "            #print image.shape\n",
    "\n",
    "            stacked_image = np.dstack((image,) * 3)\n",
    "            #print stacked_image.shape\n",
    "\n",
    "            image_folder = os.path.join(output_folder, usage)\n",
    "            if not os.path.exists(image_folder):\n",
    "                os.makedirs(image_folder)\n",
    "            image_file =  os.path.join(image_folder , str(id)+'_'+emotion+'.jpg')\n",
    "            scipy.misc.imsave(image_file, stacked_image)\n",
    "            id += 1 \n",
    "            if id % 100 == 0:\n",
    "                print('Processed {} images'.format(id))\n",
    "    \n",
    "    print(\"Finished processing {} images\".format(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "batch_size = 32\n",
    "num_epochs = 110\n",
    "input_shape = (48, 48, 1)\n",
    "verbose = 1\n",
    "num_classes = 7\n",
    "patience = 50\n",
    "base_path = 'models/'\n",
    "l2_regularization=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "regularization = l2(l2_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0626 17:16:53.655728 140735509795712 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0626 17:16:53.689574 140735509795712 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0626 17:16:53.707942 140735509795712 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0626 17:16:53.769567 140735509795712 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0626 17:16:53.770714 140735509795712 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0626 17:16:53.853193 140735509795712 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# base\n",
    "img_input = Input(input_shape)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# module 1\n",
    "residual = Conv2D(16, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module 2\n",
    "residual = Conv2D(32, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module 3\n",
    "residual = Conv2D(64, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0626 17:18:22.030035 140735509795712 deprecation_wrapper.py:119] From /anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 46, 46, 8)    72          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 46, 46, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 46, 46, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 44, 44, 8)    576         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 44, 44, 8)    32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 44, 44, 8)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 44, 44, 16)   200         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 44, 44, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 44, 44, 16)   400         activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 44, 44, 16)   64          separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 22, 22, 16)   128         activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 22, 22, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 22, 22, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 22, 22, 16)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 22, 22, 32)   656         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 22, 22, 32)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 22, 22, 32)   1312        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 22, 22, 32)   128         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 11, 11, 32)   512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 11, 11, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 11, 11, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 11, 11, 32)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 11, 11, 64)   2336        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 11, 11, 64)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 11, 11, 64)   4672        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 11, 11, 64)   256         separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 6, 6, 64)     2048        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 6, 6, 64)     0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 6, 6, 64)     256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 6, 6, 64)     0           max_pooling2d_3[0][0]            \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 6, 6, 32)     2624        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 6, 6, 32)     128         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 6, 6, 32)     0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 6, 6, 32)     1312        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 6, 6, 32)     128         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 3, 3, 32)     2048        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 32)     0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 3, 3, 32)     128         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 3, 3, 32)     0           max_pooling2d_4[0][0]            \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 3, 3, 64)     2336        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 3, 3, 64)     256         separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 3, 3, 64)     0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 3, 3, 64)     4672        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 3, 3, 64)     256         separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 2, 2, 64)     2048        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 2, 2, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 2, 2, 64)     256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 2, 2, 64)     0           max_pooling2d_5[0][0]            \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 2, 2, 128)    8768        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 2, 2, 128)    512         separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 2, 2, 128)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 2, 2, 128)    17536       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 2, 2, 128)    512         separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 1, 1, 128)    8192        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 128)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1, 1, 128)    512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 1, 1, 128)    0           max_pooling2d_6[0][0]            \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 1, 1, 7)      8071        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 7)            0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 7)            0           global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 74,615\n",
      "Trainable params: 72,567\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# module 4\n",
    "residual = Conv2D(128, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "x = Conv2D(num_classes, (3, 3), padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Activation('softmax',name='predictions')(x)\n",
    " \n",
    "model = Model(img_input, output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "898/897 [==============================] - 272s 303ms/step - loss: 1.7790 - acc: 0.3183 - val_loss: 1.9256 - val_acc: 0.3025\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.92561, saving model to models/_mini_XCEPTION.01-0.30.hdf5\n",
      "Epoch 2/110\n",
      "898/897 [==============================] - 268s 298ms/step - loss: 1.5715 - acc: 0.3989 - val_loss: 1.4918 - val_acc: 0.4352\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.92561 to 1.49177, saving model to models/_mini_XCEPTION.02-0.44.hdf5\n",
      "Epoch 3/110\n",
      "898/897 [==============================] - 265s 295ms/step - loss: 1.4490 - acc: 0.4500 - val_loss: 1.3838 - val_acc: 0.4710\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.49177 to 1.38376, saving model to models/_mini_XCEPTION.03-0.47.hdf5\n",
      "Epoch 4/110\n",
      "898/897 [==============================] - 267s 297ms/step - loss: 1.3725 - acc: 0.4825 - val_loss: 1.3460 - val_acc: 0.4944\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.38376 to 1.34601, saving model to models/_mini_XCEPTION.04-0.49.hdf5\n",
      "Epoch 5/110\n",
      "898/897 [==============================] - 290s 323ms/step - loss: 1.3265 - acc: 0.4999 - val_loss: 1.3136 - val_acc: 0.4996\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.34601 to 1.31363, saving model to models/_mini_XCEPTION.05-0.50.hdf5\n",
      "Epoch 6/110\n",
      "898/897 [==============================] - 276s 307ms/step - loss: 1.2858 - acc: 0.5147 - val_loss: 1.3384 - val_acc: 0.4889\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.31363\n",
      "Epoch 7/110\n",
      "898/897 [==============================] - 263s 293ms/step - loss: 1.2543 - acc: 0.5279 - val_loss: 1.3221 - val_acc: 0.4873\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.31363\n",
      "Epoch 8/110\n",
      "898/897 [==============================] - 268s 298ms/step - loss: 1.2271 - acc: 0.5382 - val_loss: 1.2061 - val_acc: 0.5446\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.31363 to 1.20612, saving model to models/_mini_XCEPTION.08-0.54.hdf5\n",
      "Epoch 9/110\n",
      "898/897 [==============================] - 275s 306ms/step - loss: 1.2110 - acc: 0.5447 - val_loss: 1.1496 - val_acc: 0.5582\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.20612 to 1.14960, saving model to models/_mini_XCEPTION.09-0.56.hdf5\n",
      "Epoch 10/110\n",
      "898/897 [==============================] - 274s 305ms/step - loss: 1.1931 - acc: 0.5537 - val_loss: 1.3395 - val_acc: 0.4916\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.14960\n",
      "Epoch 11/110\n",
      "898/897 [==============================] - 305s 339ms/step - loss: 1.1749 - acc: 0.5585 - val_loss: 1.2511 - val_acc: 0.5247\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.14960\n",
      "Epoch 12/110\n",
      "898/897 [==============================] - 280s 312ms/step - loss: 1.1631 - acc: 0.5655 - val_loss: 1.2876 - val_acc: 0.5146\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.14960\n",
      "Epoch 13/110\n",
      "898/897 [==============================] - 273s 304ms/step - loss: 1.1465 - acc: 0.5694 - val_loss: 1.1573 - val_acc: 0.5632\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.14960\n",
      "Epoch 14/110\n",
      "898/897 [==============================] - 273s 304ms/step - loss: 1.1357 - acc: 0.5739 - val_loss: 1.1653 - val_acc: 0.5559\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.14960\n",
      "Epoch 15/110\n",
      "898/897 [==============================] - 272s 303ms/step - loss: 1.1255 - acc: 0.5808 - val_loss: 1.1191 - val_acc: 0.5704\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.14960 to 1.11909, saving model to models/_mini_XCEPTION.15-0.57.hdf5\n",
      "Epoch 16/110\n",
      "898/897 [==============================] - 273s 304ms/step - loss: 1.1143 - acc: 0.5820 - val_loss: 1.1570 - val_acc: 0.5582\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.11909\n",
      "Epoch 17/110\n",
      "898/897 [==============================] - 2397s 3s/step - loss: 1.1063 - acc: 0.5866 - val_loss: 1.1379 - val_acc: 0.5663\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.11909\n",
      "Epoch 18/110\n",
      "898/897 [==============================] - 273s 304ms/step - loss: 1.0948 - acc: 0.5913 - val_loss: 1.1939 - val_acc: 0.5433\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.11909\n",
      "Epoch 19/110\n",
      "898/897 [==============================] - 288s 320ms/step - loss: 1.0915 - acc: 0.5899 - val_loss: 1.1430 - val_acc: 0.5698\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.11909\n",
      "Epoch 20/110\n",
      "898/897 [==============================] - 325s 362ms/step - loss: 1.0807 - acc: 0.5946 - val_loss: 1.2097 - val_acc: 0.5432\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.11909\n",
      "Epoch 21/110\n",
      "898/897 [==============================] - 308s 343ms/step - loss: 1.0787 - acc: 0.5979 - val_loss: 1.2936 - val_acc: 0.5380\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.11909\n",
      "Epoch 22/110\n",
      "898/897 [==============================] - 313s 348ms/step - loss: 1.0713 - acc: 0.5983 - val_loss: 1.1331 - val_acc: 0.5763\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.11909\n",
      "Epoch 23/110\n",
      "898/897 [==============================] - 306s 341ms/step - loss: 1.0633 - acc: 0.5988 - val_loss: 1.1290 - val_acc: 0.5769\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.11909\n",
      "Epoch 24/110\n",
      "898/897 [==============================] - 295s 329ms/step - loss: 1.0587 - acc: 0.6037 - val_loss: 1.0850 - val_acc: 0.5929\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.11909 to 1.08498, saving model to models/_mini_XCEPTION.24-0.59.hdf5\n",
      "Epoch 25/110\n",
      "898/897 [==============================] - 280s 311ms/step - loss: 1.0532 - acc: 0.6019 - val_loss: 1.1052 - val_acc: 0.5885\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.08498\n",
      "Epoch 26/110\n",
      "898/897 [==============================] - 272s 303ms/step - loss: 1.0424 - acc: 0.6100 - val_loss: 1.1015 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.08498\n",
      "Epoch 27/110\n",
      "898/897 [==============================] - 271s 302ms/step - loss: 1.0377 - acc: 0.6145 - val_loss: 1.1201 - val_acc: 0.5763\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.08498\n",
      "Epoch 28/110\n",
      "898/897 [==============================] - 282s 314ms/step - loss: 1.0385 - acc: 0.6102 - val_loss: 1.0747 - val_acc: 0.5935\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.08498 to 1.07465, saving model to models/_mini_XCEPTION.28-0.59.hdf5\n",
      "Epoch 29/110\n",
      "898/897 [==============================] - 304s 338ms/step - loss: 1.0269 - acc: 0.6159 - val_loss: 1.0731 - val_acc: 0.5911\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.07465 to 1.07307, saving model to models/_mini_XCEPTION.29-0.59.hdf5\n",
      "Epoch 30/110\n",
      "898/897 [==============================] - 308s 343ms/step - loss: 1.0229 - acc: 0.6159 - val_loss: 1.0955 - val_acc: 0.5879\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.07307\n",
      "Epoch 31/110\n",
      "898/897 [==============================] - 294s 327ms/step - loss: 1.0198 - acc: 0.6197 - val_loss: 1.0988 - val_acc: 0.5896\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.07307\n",
      "Epoch 32/110\n",
      "898/897 [==============================] - 322s 358ms/step - loss: 1.0122 - acc: 0.6198 - val_loss: 1.0578 - val_acc: 0.6006\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.07307 to 1.05778, saving model to models/_mini_XCEPTION.32-0.60.hdf5\n",
      "Epoch 33/110\n",
      "898/897 [==============================] - 351s 391ms/step - loss: 1.0124 - acc: 0.6223 - val_loss: 1.0593 - val_acc: 0.5982\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.05778\n",
      "Epoch 34/110\n",
      "898/897 [==============================] - 315s 350ms/step - loss: 1.0103 - acc: 0.6247 - val_loss: 1.0950 - val_acc: 0.5919\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.05778\n",
      "Epoch 35/110\n",
      "898/897 [==============================] - 326s 364ms/step - loss: 1.0032 - acc: 0.6243 - val_loss: 1.0926 - val_acc: 0.5924\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.05778\n",
      "Epoch 36/110\n",
      "898/897 [==============================] - 296s 329ms/step - loss: 1.0062 - acc: 0.6231 - val_loss: 1.1035 - val_acc: 0.5853\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.05778\n",
      "Epoch 37/110\n",
      "898/897 [==============================] - 290s 323ms/step - loss: 0.9995 - acc: 0.6280 - val_loss: 1.0872 - val_acc: 0.5901\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.05778\n",
      "Epoch 38/110\n",
      "898/897 [==============================] - 279s 311ms/step - loss: 1.0001 - acc: 0.6268 - val_loss: 1.0891 - val_acc: 0.5897\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.05778\n",
      "Epoch 39/110\n",
      "898/897 [==============================] - 280s 312ms/step - loss: 0.9834 - acc: 0.6334 - val_loss: 1.0897 - val_acc: 0.5865\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.05778\n",
      "Epoch 40/110\n",
      "898/897 [==============================] - 289s 322ms/step - loss: 0.9942 - acc: 0.6296 - val_loss: 1.1115 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.05778\n",
      "Epoch 41/110\n",
      "898/897 [==============================] - 306s 341ms/step - loss: 0.9810 - acc: 0.6318 - val_loss: 1.0539 - val_acc: 0.6017\n",
      "\n",
      "Epoch 00041: val_loss improved from 1.05778 to 1.05385, saving model to models/_mini_XCEPTION.41-0.60.hdf5\n",
      "Epoch 42/110\n",
      "898/897 [==============================] - 306s 341ms/step - loss: 0.9853 - acc: 0.6307 - val_loss: 1.0572 - val_acc: 0.6082\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.05385\n",
      "Epoch 43/110\n",
      "898/897 [==============================] - 314s 349ms/step - loss: 0.9814 - acc: 0.6320 - val_loss: 1.0609 - val_acc: 0.6043\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.05385\n",
      "Epoch 44/110\n",
      "898/897 [==============================] - 297s 331ms/step - loss: 0.9744 - acc: 0.6359 - val_loss: 1.0988 - val_acc: 0.5836\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.05385\n",
      "Epoch 45/110\n",
      "898/897 [==============================] - 328s 365ms/step - loss: 0.9726 - acc: 0.6350 - val_loss: 1.0594 - val_acc: 0.6037\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.05385\n",
      "Epoch 46/110\n",
      " 91/897 [==>...........................] - ETA: 4:16 - loss: 0.9513 - acc: 0.6398"
     ]
    }
   ],
   "source": [
    "# callbacks\n",
    "\n",
    "#xtrain, xtest,ytrain,ytest =\n",
    "log_file_path = base_path + 'emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    " \n",
    "model.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),\n",
    "                        steps_per_epoch=len(xtrain) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=(xtest,ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
